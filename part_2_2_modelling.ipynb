{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Using Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Warnings imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printed barrier function\n",
    "def barrier():\n",
    "    print(\"\\n <<<\",\"-\"*50,\">>> \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " <<< -------------------------------------------------- >>> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we import data from the Part 1 and Part 2-1 notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"models/X_train.pickle\",\"rb\")\n",
    "X_train = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"models/X_test.pickle\",\"rb\")\n",
    "X_test = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"models/y_train.pickle\",\"rb\")\n",
    "y_train = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"models/y_test.pickle\",\"rb\")\n",
    "y_test = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280000, 500000)\n",
      "(320000, 500000)\n",
      "(1280000,)\n",
      "(320000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from sparse matrix to sparse tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural networks work best with sparse tensors that have had their indexes re-ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scipy sparse matrix to sparse tensor in tensorflow\n",
    "def convert(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the sake of memory efficiency, we will train the models in batches of 100,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model using batches of X_train\n",
    "def batch_func(model):\n",
    "    start = 0\n",
    "    end = 100000\n",
    "    bs = 100000\n",
    "    outputs = {\"accuracy\":[], \"loss\":[]}\n",
    "\n",
    "    while (start < X_train.shape[0]):\n",
    "        x = convert(X_train[start:end])\n",
    "        y = y_train[start:end]\n",
    "        o = model.train_on_batch(x, y, return_dict=True)\n",
    "        outputs[\"loss\"].append(o[\"loss\"])\n",
    "        outputs[\"accuracy\"].append(o[\"accuracy\"])\n",
    "        start += bs\n",
    "        end += bs\n",
    "    \n",
    "    return model, {\"loss\":outputs[\"loss\"][-1], \"accuracy\":outputs[\"accuracy\"][-1],}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will evaluate models using training and testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate(name, model):\n",
    "    print(f\"EVALUATION OF {name} MODEL.\")\n",
    "    start = time.time()\n",
    "    model, metrics = batch_func(model)\n",
    "    print(f\"The training run time was {time.time() - start} seconds.\\n\\n\")\n",
    "    print(\"Training metrics:\\n\", metrics, \"\\n\\n\")\n",
    "    print(\"Testing metrics:\\n\", model.evaluate(convert(X_test), y_test, return_dict=True))\n",
    "    barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e7e167507953da952e50257aae77f36ec28e6cc"
   },
   "source": [
    "### Baseline model\n",
    "- We will use a base model with 2 densely connected layers of 64 hidden elements.\n",
    "- The input_shape for the first layer is equal to the number of features from our vectorizer which is 500,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base model\n",
    "def base():\n",
    "    base_model = models.Sequential()\n",
    "    base_model.add(layers.Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "    base_model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    base_model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    base_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION OF BASE MODEL.\n",
      "The training run time was 15.164917707443237 seconds.\n",
      "\n",
      "\n",
      "Training metrics:\n",
      " {'accuracy': 0.8520749807357788, 'loss': 0.3611500859260559} \n",
      "\n",
      "\n",
      "10000/10000 [==============================] - 16s 2ms/step - loss: 0.4081 - accuracy: 0.8184\n",
      "Testing metrics:\n",
      " {'loss': 0.4081416130065918, 'accuracy': 0.8183968663215637}\n",
      "\n",
      " <<< -------------------------------------------------- >>> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"BASE\", base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0cc919808080166a9045d241e991f9a521e8abc8"
   },
   "source": [
    "### Potential of overfitting\n",
    "- The base model produced decent results of 85% training accuracy and 82% test accuracy\n",
    "- However, there is a potential of overfitting. To explore this, we will use 3 avenues:\n",
    "    - Reduce the network's size\n",
    "    - Add regularization\n",
    "    - Add dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6768d95a28ff456f33546660512f3ffb398a94d9"
   },
   "source": [
    "#### Reducing the network's size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "_uuid": "268a0b46d93dae85e0b536a69a3efdf6eaea402b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_46 (Dense)            (None, 32)                16000032  \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,000,065\n",
      "Trainable params: 16,000,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reduced_model = models.Sequential()\n",
    "reduced_model.add(layers.Dense(32, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "reduced_model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "display(reduced_model.summary())\n",
    "reduced_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION OF REDUCED MODEL.\n",
      "The training run time was 11.529033899307251 seconds.\n",
      "\n",
      "\n",
      "Training metrics:\n",
      " {'accuracy': 0.7823125123977661, 'loss': 0.6719483137130737} \n",
      "\n",
      "\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 0.6696 - accuracy: 0.7835\n",
      "Testing metrics:\n",
      " {'loss': 0.6695919036865234, 'accuracy': 0.7834843993186951}\n",
      "\n",
      " <<< -------------------------------------------------- >>> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"REDUCED\", reduced_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- Reducing the network size did not improve the training or test accuracy over the base model.\n",
    "- Larger models tend to have a better \"explaining\" effect on input data.\n",
    "- We can consider other reduction steps like maintaining the original number of layers while reducing only the number of hidden nodes per layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd65f73d772539b52c492ab60df5d07cf08a236b"
   },
   "source": [
    "#### Adding regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "_uuid": "17e384daee482847293925c5c98697eb93b10c81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 64)                32000064  \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,004,289\n",
      "Trainable params: 32,004,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg_model = models.Sequential()\n",
    "reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation=\"relu\",\n",
    "                           input_shape=(X_train.shape[1],)))\n",
    "reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation=\"relu\"))\n",
    "reg_model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "display(reg_model.summary())\n",
    "reg_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION OF REGULARIZED MODEL.\n",
      "The training run time was 15.106367588043213 seconds.\n",
      "\n",
      "\n",
      "Training metrics:\n",
      " {'accuracy': 0.768875002861023, 'loss': 0.7476487159729004} \n",
      "\n",
      "\n",
      "10000/10000 [==============================] - 185s 19ms/step - loss: 0.7417 - accuracy: 0.7692\n",
      "Testing metrics:\n",
      " {'loss': 0.741715133190155, 'accuracy': 0.7692124843597412}\n",
      "\n",
      " <<< -------------------------------------------------- >>> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"REGULARIZED\", reg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- Adding regularization did not improve the training or test accuracy over the base model.\n",
    "- However, this is not entirely surprising or entirely a negative thing. Regularization tends to reduce training accuracy and may be reducing test accuracy because of the coefficient used.\n",
    "- Ideally, cross validation will determine the best value to use for regularization. We will keep that in mind for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "34c7ba86080b858475e08ae9652d9e7e1cb85dd4"
   },
   "source": [
    "#### Adding dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "_uuid": "148084e8fdbae9eb8aacb16f63fb5ad1afa715c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_60 (Dense)            (None, 64)                32000064  \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,004,289\n",
      "Trainable params: 32,004,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drop_model = models.Sequential()\n",
    "drop_model.add(layers.Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "drop_model.add(layers.Dropout(0.5))\n",
    "drop_model.add(layers.Dense(64, activation=\"relu\"))\n",
    "drop_model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "display(drop_model.summary())\n",
    "drop_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION OF DROPOUT_LAYER MODEL.\n",
      "The training run time was 27.643298387527466 seconds.\n",
      "\n",
      "\n",
      "Training metrics:\n",
      " {'loss': 0.6679771542549133, 'accuracy': 0.773562490940094} \n",
      "\n",
      "\n",
      "10000/10000 [==============================] - 16s 2ms/step - loss: 0.6607 - accuracy: 0.7842\n",
      "Testing metrics:\n",
      " {'loss': 0.6606889367103577, 'accuracy': 0.7842468619346619}\n",
      "\n",
      " <<< -------------------------------------------------- >>> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"DROPOUT_LAYER\", drop_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- Adding drop out layers did not improve the training or test accuracy over the base model.\n",
    "- Dropout is a regularization technique. It makes the model more robust by intentionally disregarding certain nodes.\n",
    "- However, when the network is small relative to the data set (like in our case), it can actually worsen performance and may be generally unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://80f27634-d6f5-464f-b87b-4912cd158457/assets\n",
      "INFO:tensorflow:Assets written to: ram://656e0a37-37ec-4927-8e0b-dd92d35a46ba/assets\n",
      "INFO:tensorflow:Assets written to: ram://bf8aba9a-1f62-49b0-9d88-fa7e1115c607/assets\n",
      "INFO:tensorflow:Assets written to: ram://830ae7c5-80dd-47cc-a860-2397d3686d4d/assets\n"
     ]
    }
   ],
   "source": [
    "file = open(\"models/base_dl_model.pickle\",\"wb\")\n",
    "pickle.dump(base_model, file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"models/reduced_dl_model.pickle\",\"wb\")\n",
    "pickle.dump(reduced_model, file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"models/reg_dl_model.pickle\",\"wb\")\n",
    "pickle.dump(reg_model, file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"models/drop_dl_model.pickle\",\"wb\")\n",
    "pickle.dump(drop_model, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e70f3c62923a0d30b88789f841939c8abdc75c9"
   },
   "source": [
    "## Final conclusions and potential next steps\n",
    "- The base model is the most accuracy with 85% training accuracy and 82% testing accuracy.\n",
    "- This is comparable to but slightly lower than what we achieved with the Logistic Regression Model in the previous notebook (87% training accuracy and 83% test accuracy.\n",
    "- We will proceed with the Logistic Regression model although there is further scope to explore more deep learning models (RSSs, CNNs, LSTM etc) as well as cross-validate the non-network models\n",
    "- Part 2-1 and Part 2-2 are contained with the same project folder as this Part 1.\n",
    "- See https://github.com/Daolaiya/Data-Science-Portfolio/tree/main/Project%203"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "355px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
